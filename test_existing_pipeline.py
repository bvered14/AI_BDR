#!/usr/bin/env python3
"""
Test Existing Pipeline
Tests AI enrichment and Airtable pipeline without fetching new Apollo leads
"""
import os
import sys
from dotenv import load_dotenv

# Add the current directory to the path so we can import our modules
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from process_leads import LeadProcessor
    from outreach import OutreachGenerator
    from airtable_api import AirtableAPI
    from config import Config
except ImportError as e:
    print(f"âŒ Import error: {e}")
    print("Make sure you're running this from the project root directory")
    sys.exit(1)

# Load environment variables
load_dotenv()

def test_existing_pipeline():
    """Test the pipeline with existing data"""
    print("ğŸš€ Testing Existing Pipeline (AI + Airtable)")
    print("=" * 50)
    
    # Validate configuration
    try:
        Config.validate_required()
        print("âœ… Configuration validated")
    except Exception as e:
        print(f"âŒ Configuration error: {e}")
        return
    
    # Test Airtable connection
    print("\nğŸ” Testing Airtable connection...")
    try:
        airtable = AirtableAPI()
        airtable.test_connection()
        print("âœ… Airtable connection successful")
    except Exception as e:
        print(f"âŒ Airtable connection failed: {e}")
        return
    
    # Test AI enrichment (without Apollo data)
    print("\nğŸ¤– Testing AI enrichment...")
    try:
        processor = LeadProcessor()
        print("âœ… Lead processor initialized")
        
        # Test with sample data
        sample_lead = {
            'first_name': 'John',
            'last_name': 'Doe',
            'title': 'CTO',
            'company_name': 'Test Company',
            'company_industry': 'Technology',
            'company_size': 200,
            'region': 'North America'
        }
        
        print(f"   ğŸ“ Testing with sample lead: {sample_lead['first_name']} {sample_lead['last_name']}")
        
        # Test scoring
        score, reasons = processor.calculate_total_score(sample_lead)
        print(f"   ğŸ¯ Score: {score:.2f}")
        print(f"   ğŸ“Š Reasons: {reasons}")
        
        # Test email generation
        email_gen = OutreachGenerator()
        email_data = email_gen.generate_personalized_email(sample_lead)
        print(f"   ğŸ“§ Email generated: {len(email_data.get('body', ''))} characters")
        print(f"   ğŸ“‹ Subject: {email_data.get('subject', 'No subject')}")
        print(f"   ğŸ“‹ Preview: {email_data.get('body', '')[:100]}...")
        
        print("âœ… AI enrichment working")
        
    except Exception as e:
        print(f"âŒ AI enrichment failed: {e}")
        return
    
    print("\nğŸ¯ Pipeline Test Results:")
    print("=" * 50)
    print("âœ… Configuration: Working")
    print("âœ… Airtable: Working")
    print("âœ… AI Enrichment: Working")
    print("âœ… Email Generation: Working")
    print("\nğŸš€ Your pipeline is ready for the next step!")
    print("ğŸ’¡ Next: Use your existing Apollo list data to test the full flow")

if __name__ == "__main__":
    test_existing_pipeline()
